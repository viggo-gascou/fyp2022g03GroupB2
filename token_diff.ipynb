{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "a8e02646-f16e-420e-8404-b68199036185",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1650540493737,
    "source_hash": "77ed952d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex as re\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cell_id": "d1e69e3e16184d709e26192eb61ed379",
    "deepnote_cell_height": 297,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 41,
    "execution_start": 1650540561776,
    "source_hash": "152d825f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"pattern.txt\", \"r\") as f:\n",
    "    pat = re.compile(f.read())\n",
    "\n",
    "\n",
    "with open(\"tweeteval/datasets/irony/train_text.txt\", \"r\") as f:\n",
    "    irony = f.read()\n",
    "\n",
    "corp_irony = re.findall(pat, irony)\n",
    "\n",
    "with open(\"tweeteval/datasets/stance/climate/train_text.txt\", \"r\") as f:\n",
    "    stance_climate = f.read()\n",
    "\n",
    "corp_stance = re.findall(pat, stance_climate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cell_id": "b60c16104582459283244bc4d87ec48a",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     611
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 214,
    "execution_start": 1650540682440,
    "source_hash": "1285d9f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk_token = TweetTokenizer()\n",
    "nltk_irony = nltk_token.tokenize(irony)\n",
    "nltk_stance = nltk_token.tokenize(stance_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "a7d88ed6c2104d9b9476cb48a779a8f0",
    "deepnote_cell_height": 152.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2131,
    "execution_start": 1650540720906,
    "owner_user_id": "b6cc7348-6fc5-4d35-ad0e-eb6f6874910a",
    "source_hash": "6e2daaf9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = difflib.Differ()\n",
    "result = list(d.compare(corp_irony, nltk_irony))\n",
    "\n",
    "# same_lst = [line for line in result if line.startswith(\" \")]\n",
    "# nltk_lst = [line for line in result if line.startswith(\"+\")]\n",
    "# our_lst = [line for line in result if line.startswith(\"-\")]\n",
    "# not_lst = [line for line in result if line.startswith(\"?\")]\n",
    "\n",
    "# same_c = len(same_lst)\n",
    "# uniq_nltk = len(nltk_lst)\n",
    "# uniq_our = len(our_lst)\n",
    "# not_present = len(not_lst)\n",
    "\n",
    "# print(same_c, uniq_nltk, uniq_our, not_present)\n",
    "\n",
    "# print(uniq_our + uniq_nltk)\n",
    "\n",
    "# print(our_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"\"\"All I can say is I'm lucky. \n",
    "#notcies #eu Liverpool workers \"miscarriage of justice\" victims \n",
    "@user yesss! What makes it worse is when they look gorgeous and say \"I just got my face beat!\" Wtf?  #Contradiction \n",
    "Xmas on the blog feat @user and @user * Read our story and share the LOVE ‚ù§Ô∏è Click the link... \n",
    "Watching the move 'Begin Again' and the verisimilitude is overpowering, it's like I'm back in the 90s music business again.   \n",
    "This is not the moon. Pictures like the moon is made of light bulbs.  #the #moon :) \n",
    "I got ready and then got to school and parked in less than 12 minutes! #miracle \n",
    "@user yeah. So as you can see, I have great success with the ladies! And I'm totally excited for having sex some more!  \n",
    "I don't like clowns but I'm going to be one.  \n",
    "MLS Transactions 2015 #MLS making waves again   2 b fair it will take more than 2 players to fix this \n",
    "@user I'll be a bit sweaty by the time I get to you! \n",
    " a bad game last night. Way to go Packers! \n",
    "I hope I could recover from fever today. I need to start with strama.. \n",
    "somebody wake me up early tomorrow ive been facing weird aches in my back since early december .,. and why do u think that relates madaka \n",
    "The Champions League is overrated anyway!  \n",
    "Porygon2 are  found in the www.monstermmorpg. com wild. #firemen follow @user #paint \n",
    "@user are you looking at the wrong profile picture? \n",
    "@user One day I want to travel with my bestfriend üåè‚úàÔ∏è DONE DID TRAVELED DA WORLD!! @user ‚ù§Ô∏è \n",
    "@user u simply cant win with @user if it is twitter fight!!! :-P \n",
    "Fully charged my #Anker portable charger...it lasted 1/2 an hour.  Awesome #Fail \n",
    "#Italy -- #Cabinet #approves #first #planks of #Renzi's #labour #reform. via @user \n",
    "ruling party in power#central#state#misusing their power#PM speaking only in foreign parliment#pm to visit out side india during session \n",
    "Gareth's polar opposite is a chicken-loving vegetarian üòÇüê£  #Bones @user \n",
    "Watching creepy shit before bed when alone = bad idea. Is there a spell to turn a French bulldog into a big ass bulldog? #bewareofdog  \n",
    "i do occupy rent free space in his cranial cavity LOL @user \n",
    "Had to take a #PatioPics - snow falling still. This was totally clear when I went to sleep. #WCCO \n",
    "August has the most birthdays, February has the least and most of the serial killers are born in November!||-so dont mess up with me|#nov26 \n",
    "Lol RT @user Wouldn't surprise me if Soldado bangs in a hatrick and we win 0-3 against Chelsea tonight .. The legend is back \n",
    "My husband thinks I'm crazy because I taped my tape dispenser. Hehe. I'm handy like that. .... \n",
    "My secret name is lizard squad. I like to ruin people's fun time. Follow and rt to a billion and you'll have fun. #psn  #giveitup \n",
    "Tomorrow's afternoon #NFL sked in #PanamaCity area: WECP 12p #KCvsPIT, 3:25p #INDvsDAL; WPGX 12p #ATLvsNO. \n",
    "@user @user I sure hope ev1 vaccinations are up to date! #GoBolts @user @user @user \n",
    "Pulis turned down #NUFC cos he wants to spend a load of money on 30 year old journeymen. Parish wouldn't let him & neither would MA. #cpfc \n",
    "Sending best wishes to all my coworkers at the 9AM this morning  \n",
    "@user try having no internet for a month. Now I know how Ethiopians feel.  \n",
    "so, sane peoples would talk to themselves in twitter because they can't find other sane humans to talk to. that  #retweet#ifagree \n",
    "Thanks @user for connecting. Always look forward to exchange thoughts n ideas with #entrepreneur working on #green n #sustainability \n",
    "Seems as if @user wants to endorse me on LinkedIn for  - any thoughts on this from the #OMCchat crowd? \n",
    "Love being made fun of  \n",
    "@user @user @user Oh wow your talking Skype! Cool!  \n",
    "he was half of what she deserved, yet he was all that she ever wanted ,,,,  \n",
    "5:30AM 00:00 12PM\n",
    "#Christmas  #been #the #best @ West Monkseaton \n",
    "Parking meter obviously forgot to get its own parking ticket.  \n",
    "About to fuck up this Media exam  #actuallyihopeso \n",
    "well today is gonna be a great day üëå  \n",
    "Heaven help the fool who did her wrong \n",
    "Just bartered for a bottle of rum in best one, and got it down from ¬£18 to ¬£14. Happy Fucking New Year to me!! \n",
    "find ONE local PD that reported an 80% drop @user @user @user @user @user @user \n",
    "@user @user Money 4 Church|http://t.co/Q2WB7riAvK|SmartPhone APP PAYS you!|See-http://t.co/RDlRuGN0iE |Go 2: \n",
    "Well it's always a good time losing at the Bay... @user @user @user \n",
    "Welsh devolution? How's this for starters... \n",
    "I love when folks call Brady a system QB but are THE BIGGEST Peyton Manning fans. . \n",
    "@user Instead of playing the pompous \"do you know who I am card?\" , how about you actually make an educated rebuttal? \n",
    "Kind of love how I got a voicemail from my seat neighbor wondering where I was yet they constantly sell their ticket & I never ask  \n",
    "I feel a nap in my near future. #NapTime \n",
    "#AnalScreen #Exotic Exotic brunette gets her little tight butt nailed right on the office desk \n",
    "#sundayfunday #mylove #mermaidlove #newyear2015 @ Rockefeller Center \n",
    "The ever so caring @user gets to see the siege ending first. Great journalism.  \n",
    "www.google.com\n",
    "@user You truly are my son. \n",
    "hmmm. I do wonder why Astec has one fewer employee? #lol  \n",
    "#Germany -- #ECB's #Weidmann #says #German #2015 #growth #may #be #better #than #expected. via @user \n",
    "Kevin Durant with 23pts on 8-13 shooting, has this nigga been inefficient since he came.  \n",
    "This chap seems to be a bit of an over sexed out going extrovert ... Must be his overly masculine voice and demeanor.   \n",
    "Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.  \n",
    "@user so funny lolololol  \n",
    "@user @user what the hell ever. \n",
    "On my lunch break so sleepyüò¥ \n",
    "@user @user More clean OR cleaner, never more cleaner  \n",
    "Amen, that's due to them  having respect for themselves.  ;P ;p :p 8D xD \"\"\"\n",
    "\n",
    "link_line = \"\"\"https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py\n",
    "http://github .com/nltk/nltk/blob/develop/nltk/tokenize/casual.py\n",
    "www.github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py\n",
    "www.google.org\n",
    "www.google.com\n",
    "http://www.github.com/nltk/nltk/blob/develop/nltk.py\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = irony\n",
    "\n",
    "tokens = []\n",
    "unmatchable = []\n",
    "\n",
    "mybe_everything = r\"\"\"(https?[:.]?\\s?\\/\\/(?:\\s*[^\\/\\s.]+)+(?:\\s*\\.\\s*[^\\/\\s.]+)*(?:\\s*\\/\\s*[^\\/\\s]+)*)|(www?[\\..]?\\s?\\/\\/(?:\\s*[^\\/\\s.]+)+(?:\\s*\\.\\s*[^\\/\\s.]+)*(?:\\s*\\/\\s*[^\\/\\s]+)*)|(?:[<>]?[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8][<>]?|</?3)|[\\U0000263a-\\U000e007f]|\\w+[\\.,]\\S+|\\b\\d{1,3}%|(?:1)?0\\/10|(?<![!\\-:/])\\b\\d{1}\\b(?![!\\-:/])|(?<!#)\\b[A-Z]\\w+\\s[A-Z]\\p{L}\\w*\\b|w\\/|w\\/o|['\\\"]\\b\\p{L}+\\b(?:['‚Äô]\\b\\w+\\b)?(?:-\\b\\w+\\b)?['\\\"]|@?#?\\b\\p{L}+\\d*\\b(?:['‚Äô]\\b\\w+\\b)?(?:-(?!http)\\b\\w+\\b)?|([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)|(\\w+[¬¥`']\\w+)|(\\w+[\\.,]\\S+)|([\\.,?!]+)|(\\w+)|(#\\w+)|(@\\w+)|(@)|([¬¥`'\\\"].*?[¬¥`'\\\"])\"\"\"\n",
    "test = r\"\"\"(https?[:.]?\\s?\\/\\/(?:\\s*[^\\/\\s.]+)+(?:\\s*\\.\\s*[^\\/\\s.]+)*(?:\\s*\\/\\s*[^\\/\\s]+)*)|(www?[\\..]?\\s?\\/\\/(?:\\s*[^\\/\\s.]+)+(?:\\s*\\.\\s*[^\\/\\s.]+)*(?:\\s*\\/\\s*[^\\/\\s]+)*)|(?:[<>]?[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8][<>]?|</?3)|[\\U0000263a-\\U000e007f]|\\w+[\\.,]\\S+|\\b\\d{1,3}%|(?:1)?0\\/10|(?<![!\\-:/])\\b\\d{1}\\b(?![!\\-:/])|(?<!#)\\b[A-Z]\\w+\\s[A-Z]\\p{L}\\w*\\b|w\\/|w\\/o|['\\\"]\\b\\p{L}+\\b(?:['‚Äô]\\b\\w+\\b)?(?:-\\b\\w+\\b)?['\\\"]|@?#?\\b\\p{L}+\\d*\\b(?:['‚Äô]\\b\\w+\\b)?(?:-(?!http)\\b\\w+\\b)?|([A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+)|(\\d+-\\d+)|(\\d+/\\d+)|(\\d+(:?\\d+)?((AM)?(PM)?[ps]?))|[\\$\\¬£\\‚Ç¨](\\d+(?:\\.\\d{1,2})?)|(\\w+[¬¥`']\\w+)|(\\w+[\\.,]\\S+)|([\\.,?&!\\-\\*:;\\|=]+)|(\\w+)|(#\\w+)|(@\\w+)|(@)|([¬¥`'\\\"].*?[¬¥`'\\\"])\"\"\"\n",
    "\n",
    "with open(\"pattern2.txt\", \"r\") as f:\n",
    "    token_pat = re.compile(f.read())\n",
    "\n",
    "skippable_pat = re.compile(r'\\s+')\n",
    "\n",
    "# As long as there's any material left...\n",
    "while line:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match = re.search(skippable_pat, line)\n",
    "    if skippable_match and skippable_match.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        line = line[skippable_match.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match = re.search(token_pat, line)\n",
    "        if token_match and token_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens.append(line[:token_match.end()])\n",
    "            line = line[token_match.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end = len(line)\n",
    "            if skippable_match:\n",
    "                unmatchable_end = skippable_match.start()\n",
    "            if token_match:\n",
    "                unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable.append(line[:unmatchable_end])\n",
    "            line = line[unmatchable_end:]\n",
    "# tokens = re.findall(token_pat, line)\n",
    "# unmatchable = re.findall(skippable_pat, line)\n",
    "\n",
    "# print(\"\\n\" + \"MATCHED\" + \"\\n\" + \"-\"*100 + \"\\n\")\n",
    "# print(tokens)\n",
    "# print(\"\\n\" + \"UNMATCHED\" + \"\\n\" + \"-\"*100 + \"\\n\")\n",
    "# print(unmatchable)\n",
    "\n",
    "d = difflib.Differ()\n",
    "result = list(d.compare(corp_irony, nltk_irony))\n",
    "\n",
    "same_lst = [lin for lin in result if lin.startswith(\" \")]\n",
    "nltk_lst = [lin for lin in result if lin.startswith(\"+\")]\n",
    "our_lst = [lin for lin in result if lin.startswith(\"-\")]\n",
    "not_lst = [lin for lin in result if lin.startswith(\"?\")]\n",
    "\n",
    "same_c = len(same_lst)\n",
    "uniq_nltk = len(nltk_lst)\n",
    "uniq_our = len(our_lst)\n",
    "not_present = len(not_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amount of same tokens: 37288\n",
      "Unique tokens in our tokenizer: 1053\n",
      "Unique tokens in nltk tokenizer: 7734\n",
      "Amount of tokens not present in either tokenizer: 319\n",
      "Length of full irony text: 229674\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Amount of same tokens: {same_c}\n",
    "Unique tokens in our tokenizer: {uniq_our}\n",
    "Unique tokens in nltk tokenizer: {uniq_nltk}\n",
    "Amount of tokens not present in either tokenizer: {not_present}\n",
    "Length of full irony text: {len(irony)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = difflib.Differ()\n",
    "result2 = list(d.compare(tokens, nltk_irony))\n",
    "\n",
    "same_lst2 = [lin2 for lin2 in result2 if lin2.startswith(\" \")]\n",
    "nltk_lst2 = [lin2 for lin2 in result2 if lin2.startswith(\"+\")]\n",
    "our_lst2 = [lin2 for lin2 in result2 if lin2.startswith(\"-\")]\n",
    "not_lst2 = [lin2 for lin2 in result2 if lin2.startswith(\"?\")]\n",
    "\n",
    "same_c2 = len(same_lst2)\n",
    "uniq_nltk2 = len(nltk_lst2)\n",
    "uniq_our2 = len(our_lst2)\n",
    "not_present2 = len(not_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amount of same tokens: 39913\n",
      "Unique tokens in our tokenizer: 2092\n",
      "Unique tokens in nltk tokenizer: 5109\n",
      "Amount of tokens not present in either tokenizer: 639\n",
      "Length of full irony text: 229674\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Amount of same tokens: {same_c2}\n",
    "Unique tokens in our tokenizer: {uniq_our2}\n",
    "Unique tokens in nltk tokenizer: {uniq_nltk2}\n",
    "Amount of tokens not present in either tokenizer: {not_present2}\n",
    "Length of full irony text: {len(irony)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seeing',\n",
       " 'ppl',\n",
       " 'walking',\n",
       " 'w/',\n",
       " 'crutches',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " 'look',\n",
       " 'for',\n",
       " 'the',\n",
       " 'girl',\n",
       " 'with',\n",
       " 'the',\n",
       " 'broken',\n",
       " 'smile',\n",
       " 'ask',\n",
       " 'her',\n",
       " 'if',\n",
       " 'she',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'while',\n",
       " 'and',\n",
       " 'she',\n",
       " 'will',\n",
       " 'be',\n",
       " 'loved',\n",
       " 'Now',\n",
       " 'I',\n",
       " 'remember',\n",
       " 'why',\n",
       " 'I',\n",
       " 'buy',\n",
       " 'books',\n",
       " 'online',\n",
       " '@user',\n",
       " '#servicewithasmile',\n",
       " '@user',\n",
       " '@user',\n",
       " 'So',\n",
       " 'is',\n",
       " 'he',\n",
       " 'banded',\n",
       " 'from',\n",
       " 'wearing',\n",
       " 'the',\n",
       " 'clothes',\n",
       " '#Karma',\n",
       " 'Just',\n",
       " 'found',\n",
       " 'out',\n",
       " 'there',\n",
       " 'are',\n",
       " 'Etch',\n",
       " 'A',\n",
       " 'Sketch',\n",
       " 'apps',\n",
       " '#oldschool',\n",
       " '#notoldschool',\n",
       " 'Hey',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'witnesses',\n",
       " 'supporting',\n",
       " 'Darren',\n",
       " \"Wilson's\",\n",
       " 'story',\n",
       " 'lied',\n",
       " 'And',\n",
       " 'is',\n",
       " 'racist',\n",
       " 'Mind',\n",
       " 'blown',\n",
       " '@user',\n",
       " 'on',\n",
       " 'stage',\n",
       " 'at',\n",
       " '#flzjingleball',\n",
       " 'at',\n",
       " 'the',\n",
       " '@user',\n",
       " 'in',\n",
       " '#Tampa',\n",
       " '#iheartradio',\n",
       " 'You',\n",
       " 'know',\n",
       " \"it's\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " 'when',\n",
       " \"you're\",\n",
       " 'Garmin',\n",
       " 'resets',\n",
       " 'itself',\n",
       " 'and',\n",
       " 'you',\n",
       " 'spill',\n",
       " 'some',\n",
       " 'cinnamon',\n",
       " 'down',\n",
       " 'yourself',\n",
       " '#slowclap',\n",
       " 'Halfway',\n",
       " 'thorough',\n",
       " 'my',\n",
       " 'workday',\n",
       " 'Woooo',\n",
       " 'Would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'my',\n",
       " 'nephew',\n",
       " 'for',\n",
       " 'giving',\n",
       " 'me',\n",
       " 'his',\n",
       " 'horrible',\n",
       " 'cold',\n",
       " 'sore',\n",
       " 'throat',\n",
       " 'etc',\n",
       " 'Much',\n",
       " 'appreciated',\n",
       " 'I',\n",
       " 'forked',\n",
       " 'node',\n",
       " 'Get',\n",
       " 'ready',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " \"Where's\",\n",
       " 'my',\n",
       " 'interviews',\n",
       " '@user',\n",
       " '@user',\n",
       " '@user',\n",
       " '@user',\n",
       " \"I'm\",\n",
       " 'off',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'great-nephew',\n",
       " 'very',\n",
       " 'ill',\n",
       " 'only',\n",
       " '20',\n",
       " 'fair',\n",
       " 'If',\n",
       " 'his',\n",
       " 'Twitter',\n",
       " 'account',\n",
       " 'is',\n",
       " 'any',\n",
       " 'sign',\n",
       " 'then',\n",
       " 'this',\n",
       " 'book',\n",
       " 'looks',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'must',\n",
       " 'read',\n",
       " 'id',\n",
       " 'rather',\n",
       " 'read',\n",
       " 'twilight',\n",
       " 'Nicholas',\n",
       " 'Sparks',\n",
       " 'has',\n",
       " 'been',\n",
       " 'manipulating',\n",
       " 'women',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'guys',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'are',\n",
       " 'as',\n",
       " 'sensitive',\n",
       " 'as',\n",
       " 'the',\n",
       " 'male',\n",
       " 'characters',\n",
       " 'in',\n",
       " 'his',\n",
       " 'books',\n",
       " '@user',\n",
       " 'Can',\n",
       " 'U',\n",
       " 'use',\n",
       " 'Smartphone',\n",
       " 'http',\n",
       " 't.co',\n",
       " 'Q2WB7riAvK|SmartPhone',\n",
       " 'APP',\n",
       " 'PAYS',\n",
       " 'you',\n",
       " 'See-http',\n",
       " 't.co',\n",
       " 'RDlRuGN0iE',\n",
       " 'Go',\n",
       " '2',\n",
       " 'When',\n",
       " 'we',\n",
       " 'stop',\n",
       " 'accepting',\n",
       " 'crumbs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'love',\n",
       " 'only',\n",
       " 'then',\n",
       " 'we',\n",
       " 'can',\n",
       " 'equally',\n",
       " 'appreciate',\n",
       " 'the',\n",
       " 'silence',\n",
       " 'of',\n",
       " 'solitude',\n",
       " 'festivity',\n",
       " 'of',\n",
       " 'loving',\n",
       " '@CelticOwlWisdom',\n",
       " 'Alert',\n",
       " 'the',\n",
       " 'media',\n",
       " 'Some',\n",
       " 'guy',\n",
       " 'on',\n",
       " 'twitter',\n",
       " 'has',\n",
       " 'un-cited',\n",
       " 'proof',\n",
       " 'that',\n",
       " 'evolution',\n",
       " 'is',\n",
       " 'wrong',\n",
       " \"Let's\",\n",
       " 'get',\n",
       " 'his',\n",
       " 'Nobel',\n",
       " 'prize',\n",
       " 'ready',\n",
       " 'Shameless',\n",
       " 'accounting',\n",
       " 'firms',\n",
       " 'make',\n",
       " 'vast',\n",
       " 'sums',\n",
       " 'advising',\n",
       " 'rich',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'rip',\n",
       " 'off',\n",
       " 'taxpayers',\n",
       " 'accounting',\n",
       " 'chief|http',\n",
       " 't.co',\n",
       " '9BSDuJAKXb',\n",
       " '#POPCORN',\n",
       " 'bringing',\n",
       " 'this',\n",
       " 'at',\n",
       " 'movie',\n",
       " 'tuesday',\n",
       " 'night',\n",
       " 'Main',\n",
       " 'issue',\n",
       " 'with',\n",
       " 'the',\n",
       " 'walking',\n",
       " 'dead',\n",
       " 'you',\n",
       " 'forget',\n",
       " 'to',\n",
       " 'breathe',\n",
       " 'when',\n",
       " \"you're\",\n",
       " 'watching',\n",
       " 'So',\n",
       " 'bloody',\n",
       " 'good',\n",
       " '#WalkingDead',\n",
       " 'Need',\n",
       " 'to',\n",
       " 'get',\n",
       " 'back',\n",
       " 'in',\n",
       " 'to',\n",
       " 'college',\n",
       " '#feeling',\n",
       " '#this',\n",
       " 'No',\n",
       " 'Its',\n",
       " 'actually',\n",
       " 'not',\n",
       " 'Friday',\n",
       " 'Today',\n",
       " 'we',\n",
       " 'proudly',\n",
       " 'present',\n",
       " 'Sunday',\n",
       " 'So',\n",
       " 'GO',\n",
       " 'ENJOY',\n",
       " 'YOUR',\n",
       " 'SUNDAY',\n",
       " 'Thank',\n",
       " 'god',\n",
       " 'she',\n",
       " \"doesn't\",\n",
       " 'get',\n",
       " 'me',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'anymore',\n",
       " 'ellielippitt',\n",
       " '#barber',\n",
       " '#hairdresser',\n",
       " '#blondie',\n",
       " '@user',\n",
       " 'lol',\n",
       " 'how',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cthulhu',\n",
       " 'Funny',\n",
       " 'autocorrect',\n",
       " 'so',\n",
       " 'helpful',\n",
       " 'I',\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'be',\n",
       " 'weak',\n",
       " '#workout',\n",
       " '#motivation',\n",
       " '#fitfam',\n",
       " 'Why',\n",
       " 'YES',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'I',\n",
       " 'use',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'in',\n",
       " 'my',\n",
       " 'day',\n",
       " 'because',\n",
       " 'violence',\n",
       " 'is',\n",
       " 'illegal',\n",
       " '@user',\n",
       " '@user',\n",
       " '@user',\n",
       " 'no',\n",
       " 'duh',\n",
       " 'Is',\n",
       " 'will',\n",
       " 'owners',\n",
       " 'fire',\n",
       " 'Goodell',\n",
       " 'for',\n",
       " 'it',\n",
       " 'My',\n",
       " 'guess',\n",
       " 'he',\n",
       " 'gets',\n",
       " 'a',\n",
       " 'raise',\n",
       " '#GoodellMustGo',\n",
       " 'I',\n",
       " 'hate',\n",
       " 'plane',\n",
       " 'rides',\n",
       " 'but',\n",
       " 'I',\n",
       " 'wanna',\n",
       " 'go',\n",
       " 'to',\n",
       " 'so',\n",
       " 'many',\n",
       " 'different',\n",
       " 'countries',\n",
       " 'Stop',\n",
       " 'being',\n",
       " 'the',\n",
       " 'fan',\n",
       " 'And',\n",
       " 'be',\n",
       " 'the',\n",
       " 'person',\n",
       " 'blowing',\n",
       " 'minds',\n",
       " 'Ok',\n",
       " 'watching',\n",
       " 'how',\n",
       " 'hurt',\n",
       " 'parents',\n",
       " 'are',\n",
       " 'when',\n",
       " 'their',\n",
       " 'kids',\n",
       " 'decide',\n",
       " 'to',\n",
       " 'elope',\n",
       " 'definitely',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'change',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'about',\n",
       " 'it',\n",
       " 'Small',\n",
       " 'cozy',\n",
       " 'wedding',\n",
       " 'it',\n",
       " 'is',\n",
       " 'I',\n",
       " 'liked',\n",
       " 'a',\n",
       " '@user',\n",
       " 'video',\n",
       " 'NEW',\n",
       " 'Chocolate',\n",
       " 'Bar',\n",
       " 'Palette',\n",
       " 'Review',\n",
       " 'Semi',\n",
       " 'Sweet',\n",
       " 'vs',\n",
       " 'Original',\n",
       " '@user',\n",
       " 'Advertising',\n",
       " 'an',\n",
       " 'online',\n",
       " 'special',\n",
       " 'with',\n",
       " 'an',\n",
       " 'expiry',\n",
       " 'countdown',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'cant',\n",
       " 'order',\n",
       " 'anything',\n",
       " 'or',\n",
       " 'check',\n",
       " 'your',\n",
       " 'postcodes',\n",
       " 'validity',\n",
       " 'Love',\n",
       " 'that',\n",
       " 'I',\n",
       " 'still',\n",
       " 'have',\n",
       " 'kids',\n",
       " 'that',\n",
       " 'still',\n",
       " 'wake',\n",
       " 'up',\n",
       " 'early',\n",
       " 'on',\n",
       " 'Christmas',\n",
       " '#justkiddingIlovethem',\n",
       " 'That',\n",
       " 'moment',\n",
       " 'when',\n",
       " 'you',\n",
       " 'have',\n",
       " 'so',\n",
       " 'much',\n",
       " 'stuff',\n",
       " 'to',\n",
       " 'do',\n",
       " 'but',\n",
       " 'you',\n",
       " 'open',\n",
       " '@user',\n",
       " '#productivity',\n",
       " '#tumblr',\n",
       " 'Oh',\n",
       " 'god',\n",
       " 'I',\n",
       " 'just',\n",
       " 'so',\n",
       " 'happens',\n",
       " 'that',\n",
       " 'i',\n",
       " 'love',\n",
       " 'really',\n",
       " 'LOVE',\n",
       " 'slow',\n",
       " 'internet',\n",
       " '#slowinternet',\n",
       " 'Simply',\n",
       " 'having',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'christmas',\n",
       " 'time',\n",
       " 'D',\n",
       " 'isnt',\n",
       " 'it',\n",
       " 'the',\n",
       " 'best',\n",
       " 'when',\n",
       " 'youre',\n",
       " 'really',\n",
       " 'tired',\n",
       " 'then',\n",
       " 'when',\n",
       " 'you',\n",
       " 'finally',\n",
       " 'get',\n",
       " 'in',\n",
       " 'bed',\n",
       " 'youre',\n",
       " 'wide',\n",
       " 'awake',\n",
       " 'I',\n",
       " 'LOVE',\n",
       " 'IT',\n",
       " 'Breaking',\n",
       " 'up',\n",
       " 'with',\n",
       " 'your',\n",
       " 'girl',\n",
       " 'so',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'have',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'her',\n",
       " 'any',\n",
       " 'presents',\n",
       " '#lowbudget',\n",
       " '#smartmove',\n",
       " '#a',\n",
       " '#good',\n",
       " '#idea',\n",
       " '#butscheming',\n",
       " 'doe',\n",
       " 'Glad',\n",
       " \"there's\",\n",
       " 'not',\n",
       " 'a',\n",
       " 'typhoon',\n",
       " 'where',\n",
       " 'we',\n",
       " 'go',\n",
       " 'on',\n",
       " 'holiday',\n",
       " 'in',\n",
       " '4',\n",
       " 'weeks',\n",
       " '#fml',\n",
       " '#LOL',\n",
       " 'with',\n",
       " 'all',\n",
       " 'due',\n",
       " '#Respect',\n",
       " 'to',\n",
       " '#KobeBryant',\n",
       " 'he',\n",
       " 'is',\n",
       " '#MJ',\n",
       " '_',\n",
       " 'but',\n",
       " 'damn',\n",
       " 'near',\n",
       " '#Close',\n",
       " '#Legends',\n",
       " '@user',\n",
       " '@user',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'a',\n",
       " 'damned',\n",
       " 'thing',\n",
       " 'about',\n",
       " 'baseball',\n",
       " 'do',\n",
       " 'you',\n",
       " 'Rogers',\n",
       " 'that',\n",
       " 'smile',\n",
       " 'just',\n",
       " 'so',\n",
       " 'accurately',\n",
       " 'captures',\n",
       " 'my',\n",
       " 'reaction',\n",
       " 'when',\n",
       " 'I',\n",
       " 'receive',\n",
       " 'an',\n",
       " 'extra',\n",
       " 'high',\n",
       " 'phone',\n",
       " 'bill',\n",
       " 'All',\n",
       " 'I',\n",
       " 'can',\n",
       " 'say',\n",
       " 'is',\n",
       " \"I'm\",\n",
       " 'lucky',\n",
       " '#notcies',\n",
       " '#eu',\n",
       " 'Liverpool',\n",
       " 'workers',\n",
       " 'miscarriage',\n",
       " 'of',\n",
       " 'justice',\n",
       " 'victims',\n",
       " '@user',\n",
       " 'yesss',\n",
       " 'What',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'worse',\n",
       " 'is',\n",
       " 'when',\n",
       " 'they',\n",
       " 'look',\n",
       " 'gorgeous',\n",
       " 'and',\n",
       " 'say',\n",
       " 'I',\n",
       " 'just',\n",
       " 'got',\n",
       " 'my',\n",
       " 'face',\n",
       " 'beat',\n",
       " 'Wtf',\n",
       " '#Contradiction',\n",
       " 'Xmas',\n",
       " 'on',\n",
       " 'the',\n",
       " 'blog',\n",
       " 'feat',\n",
       " '@user',\n",
       " 'and',\n",
       " '@user',\n",
       " 'Read',\n",
       " 'our',\n",
       " 'story',\n",
       " 'and',\n",
       " 'share',\n",
       " 'the',\n",
       " 'LOVE',\n",
       " 'Ô∏è',\n",
       " 'Click',\n",
       " 'the',\n",
       " 'link',\n",
       " 'Watching',\n",
       " 'the',\n",
       " 'move',\n",
       " 'Begin',\n",
       " 'Again',\n",
       " 'and',\n",
       " 'the',\n",
       " 'verisimilitude',\n",
       " 'is',\n",
       " 'overpowering',\n",
       " \"it's\",\n",
       " 'like',\n",
       " \"I'm\",\n",
       " 'back',\n",
       " 'in',\n",
       " 'the',\n",
       " '90s',\n",
       " 'music',\n",
       " 'business',\n",
       " 'again',\n",
       " 'This',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'Pictures',\n",
       " 'like',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'is',\n",
       " 'made',\n",
       " 'of',\n",
       " 'light',\n",
       " 'bulbs',\n",
       " '#the',\n",
       " '#moon',\n",
       " 'I',\n",
       " 'got',\n",
       " 'ready',\n",
       " 'and',\n",
       " 'then',\n",
       " 'got',\n",
       " 'to',\n",
       " 'school',\n",
       " 'and',\n",
       " 'parked',\n",
       " 'in',\n",
       " 'less',\n",
       " 'than',\n",
       " '12',\n",
       " 'minutes',\n",
       " '#miracle',\n",
       " '@user',\n",
       " 'yeah',\n",
       " 'So',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'I',\n",
       " 'have',\n",
       " 'great',\n",
       " 'success',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ladies',\n",
       " 'And',\n",
       " \"I'm\",\n",
       " 'totally',\n",
       " 'excited',\n",
       " 'for',\n",
       " 'having',\n",
       " 'sex',\n",
       " 'some',\n",
       " 'more',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'like',\n",
       " 'clowns',\n",
       " 'but',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'MLS',\n",
       " 'Transactions',\n",
       " '2015',\n",
       " '#MLS',\n",
       " 'making',\n",
       " 'waves',\n",
       " 'again',\n",
       " '2',\n",
       " 'b',\n",
       " 'fair',\n",
       " 'it',\n",
       " 'will',\n",
       " 'take',\n",
       " 'more',\n",
       " 'than',\n",
       " '2',\n",
       " 'players',\n",
       " 'to',\n",
       " 'fix',\n",
       " 'this',\n",
       " '@user',\n",
       " \"I'll\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'sweaty',\n",
       " 'by',\n",
       " 'the',\n",
       " 'time',\n",
       " 'I',\n",
       " 'get',\n",
       " 'to',\n",
       " 'you',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'game',\n",
       " 'last',\n",
       " 'night',\n",
       " 'Way',\n",
       " 'to',\n",
       " 'go',\n",
       " 'Packers',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'I',\n",
       " 'could',\n",
       " 'recover',\n",
       " 'from',\n",
       " 'fever',\n",
       " 'today',\n",
       " 'I',\n",
       " 'need',\n",
       " 'to',\n",
       " 'start',\n",
       " 'with',\n",
       " 'strama',\n",
       " 'somebody',\n",
       " 'wake',\n",
       " 'me',\n",
       " 'up',\n",
       " 'early',\n",
       " 'tomorrow',\n",
       " 'ive',\n",
       " 'been',\n",
       " 'facing',\n",
       " 'weird',\n",
       " 'aches',\n",
       " 'in',\n",
       " 'my',\n",
       " 'back',\n",
       " 'since',\n",
       " 'early',\n",
       " 'december',\n",
       " 'and',\n",
       " 'why',\n",
       " 'do',\n",
       " 'u',\n",
       " 'think',\n",
       " 'that',\n",
       " 'relates',\n",
       " 'madaka',\n",
       " 'The',\n",
       " 'Champions',\n",
       " 'League',\n",
       " 'is',\n",
       " 'overrated',\n",
       " 'anyway',\n",
       " 'Porygon2',\n",
       " 'are',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'www.monstermmorpg',\n",
       " 'com',\n",
       " 'wild',\n",
       " '#firemen',\n",
       " 'follow',\n",
       " '@user',\n",
       " '#paint',\n",
       " '@user',\n",
       " 'are',\n",
       " 'you',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'profile',\n",
       " 'picture',\n",
       " '@user',\n",
       " 'One',\n",
       " 'day',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'with',\n",
       " 'my',\n",
       " 'bestfriend',\n",
       " 'Ô∏è',\n",
       " 'DONE',\n",
       " 'DID',\n",
       " 'TRAVELED',\n",
       " 'DA',\n",
       " 'WORLD',\n",
       " '@user',\n",
       " 'Ô∏è',\n",
       " '@user',\n",
       " 'u',\n",
       " 'simply',\n",
       " 'cant',\n",
       " 'win',\n",
       " 'with',\n",
       " '@user',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'twitter',\n",
       " 'fight',\n",
       " 'P',\n",
       " 'Fully',\n",
       " 'charged',\n",
       " 'my',\n",
       " '#Anker',\n",
       " 'portable',\n",
       " 'charger',\n",
       " 'it',\n",
       " 'lasted',\n",
       " '1/2',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'Awesome',\n",
       " '#Fail',\n",
       " '#Italy',\n",
       " '#Cabinet',\n",
       " '#approves',\n",
       " '#first',\n",
       " '#planks',\n",
       " 'of',\n",
       " '#Renzi',\n",
       " 's',\n",
       " '#labour',\n",
       " '#reform',\n",
       " 'via',\n",
       " '@user',\n",
       " 'ruling',\n",
       " 'party',\n",
       " 'in',\n",
       " 'power#central',\n",
       " '#state',\n",
       " '#misusing',\n",
       " 'their',\n",
       " 'power#PM',\n",
       " 'speaking',\n",
       " 'only',\n",
       " 'in',\n",
       " 'foreign',\n",
       " 'parliment#pm',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'out',\n",
       " 'side',\n",
       " 'india',\n",
       " 'during',\n",
       " 'session',\n",
       " \"Gareth's\",\n",
       " 'polar',\n",
       " 'opposite',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chicken-loving',\n",
       " 'vegetarian',\n",
       " '#Bones',\n",
       " '@user',\n",
       " 'Watching',\n",
       " 'creepy',\n",
       " 'shit',\n",
       " 'before',\n",
       " 'bed',\n",
       " 'when',\n",
       " 'alone',\n",
       " 'bad',\n",
       " 'idea',\n",
       " 'Is',\n",
       " 'there',\n",
       " 'a',\n",
       " 'spell',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'a',\n",
       " 'French',\n",
       " 'bulldog',\n",
       " 'into',\n",
       " 'a',\n",
       " 'big',\n",
       " 'ass',\n",
       " 'bulldog',\n",
       " '#bewareofdog',\n",
       " 'i',\n",
       " 'do',\n",
       " 'occupy',\n",
       " 'rent',\n",
       " 'free',\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = r\".+\"\n",
    "test2_re = re.compile(test2)\n",
    "re.findall(test2_re, irony)\n",
    "re.findall(r\"(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\", irony)\n",
    "re.findall(r\"[\\U0000263a-\\U000e007f]\", irony)\n",
    "re.findall(r\"(\\w+\\S\\w+|\\w+[\\/']?|[@#]?\\w+)\", irony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e4cdc3a5-dd4a-4d72-a71a-972cea883107' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6914812d-b1e4-461c-8d93-9080bf897be8",
  "interpreter": {
   "hash": "2ab8726f8a5aab2b3a64ab58eaac36928eade41587597130f1a3bd320d2b8b91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
